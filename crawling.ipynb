{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader as wb\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 코스피지수 크롤링\n",
    "#약 3년치 데이터를 크롤링해 옴\n",
    "start = datetime.datetime(2019, 8, 1)\n",
    "end = datetime.datetime(2022, 8, 1)\n",
    "\n",
    "# ^KS11 : 코스피\n",
    "df_null = wb.DataReader(\"^KS11\",\"yahoo\",start,end)\n",
    "\n",
    "# 결측치 제거\n",
    "df = df_null.dropna()\n",
    "\n",
    "# Close와 Adj Close는 중복되는 columns인것을 확인 함\n",
    "# Adj Close열을 제거\n",
    "df.drop([\"Adj Close\"],axis=1, inplace=True)\n",
    "\n",
    "# 새로운 칼럼 생성\n",
    "# (Price : 당일 대비 다음날 주가가 상승했으면 1, 하락했으면 0 표시)\n",
    "df['Price'] = 0\n",
    "for i in range(len(df)-1):\n",
    "    if df['Close'][i] < df['Close'][i+1]:\n",
    "        df['Price'][i] = 1\n",
    "    else:\n",
    "        df['Price'][i] = 0\n",
    "\n",
    "# columns명을 알기 쉽게 한글로 변경\n",
    "df.columns = [\"최고가\", \"최저가\" , \"시작가\", \"종가\", \"거래량\" , \"등락\"]\n",
    "\n",
    "# 파일 저장\n",
    "df.to_csv('kospi_주가데이터.csv',encoding='utf-8-sig')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 등락 값이 \"0\"인 데이터와 \"1\"인 데이터를 나눠서 리스트화\n",
    "# 여기서 리스트화 하는 이유는 다음날 주가가 하락한 경우의 뉴스 타이틀과 상승한 뉴스타이틀을 크롤링해 분류하기 위함\n",
    "price_data = pd.read_csv('kospi_주가데이터.csv')\n",
    "\n",
    "#주가가 하락한 경우의 Date 데이터 리스트 date_0\n",
    "df_0 = price_data[price_data['등락']==0]['Date']\n",
    "date_0 = []\n",
    "for i in range(0,len(df_0)):\n",
    "    date_0.append(str(df_0.tolist()[i])[:10].replace('-',''))\n",
    "\n",
    "#주가가 상승한 경우의 Date 데이터 리스트 date_1\n",
    "df_1 = price_data[price_data['등락']==1]['Date']\n",
    "date_1 = []\n",
    "for i in range(0,len(df_1)):\n",
    "    date_1.append(str(df_1.tolist()[i])[:10].replace('-',''))\n",
    "\n",
    "date_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#뉴스타이틀 크롤링을 위한 모듈 import\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib import parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 팍스넷 크롤링 함수 paxnet_news_title\n",
    "result_list = []\n",
    "error_cnt = 0\n",
    "\n",
    "def paxnet_news_title(dates):\n",
    "    base_url = 'http://www.paxnet.co.kr/news/much?newsSetId=4667&currentPageNo={}&genDate={}&objId=N4667'\n",
    "    headers = {\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    for date in dates:\n",
    "        for page in range(1, 3):\n",
    "            url = base_url.format(page, date)\n",
    "            res = requests.get(url, headers=headers)\n",
    "            if res.status_code == 200:\n",
    "                soup = BeautifulSoup(res.text)\n",
    "                title_list = soup.select('ul.thumb-list li')\n",
    "                for title in title_list:\n",
    "                    try:\n",
    "                        news_title = title.select_one('dl.text > dt').text.strip()\n",
    "                        #\"[문자]\" 가 붙은 제거\n",
    "                        #find 함수는 문자열안에 지정한 문자가 있을경우 그 위치(index)를 int로 반환해준다.\n",
    "                        #지정한 문자를 찾지 못할 경우 -1 을 반환 \n",
    "                        find_1=news_title.find(\"[\")\n",
    "                        find_2=news_title.find(\"]\")\n",
    "\n",
    "                        # find함수의 결과가 -1가 아닌 경우(문자열안에 \"[문자]\"가 있다.)\n",
    "                        # 그 위치를 찾아 제거하고 result_list에 추가\n",
    "                        if find_1 != -1:\n",
    "                            slice_news_title=news_title[find_1:find_2+1]\n",
    "                            result_list.append([news_title.strip(slice_news_title).strip()])\n",
    "                        #find함수의 결과가 -1 일경우(문자열안에 \"[문자]\"가 없다.)\n",
    "                        #바로 result_list에 추가\n",
    "                        elif find_1 == -1:\n",
    "                            result_list.append([news_title])\n",
    "                    except:\n",
    "                        error_cnt += 1\n",
    "paxnet_news_title(date_0)\n",
    "title_df_0 = pd.DataFrame(result_list, columns=['뉴스제목'])\n",
    "title_df_0['주가변동'] = 0\n",
    "result_list = []\n",
    "\n",
    "paxnet_news_title(date_1)\n",
    "title_df_1 = pd.DataFrame(result_list, columns=['뉴스제목'])\n",
    "title_df_1['주가변동'] = 1\n",
    "result_list = []\n",
    "\n",
    "\n",
    "\n",
    "#팍스넷 크롤링 데이터 합쳐 csv파일로 만들기\n",
    "title_df = pd.concat([title_df_0, title_df_1])\n",
    "#중복 데이터 삭제 \n",
    "del_title_df = title_df.drop_duplicates(['뉴스제목'])\n",
    "#데이터프레임 저장 \n",
    "del_title_df.to_csv('팍스넷_뉴스타이틀.csv', index=False, encoding='utf-8-sig')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 크롤링 함수 naver_news_title\n",
    "result_list = []\n",
    "error_cnt = 0\n",
    "def naver_news_title(dates):\n",
    "    base_url = 'https://finance.naver.com/news/mainnews.naver?date={}&page={}'\n",
    "    headers = {\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'\n",
    "    }\n",
    "    for date in dates:\n",
    "        for page in range(1, 3):\n",
    "            url = base_url.format(date,page)\n",
    "            res = requests.get(url, headers=headers)\n",
    "            if res.status_code == 200:\n",
    "                soup = BeautifulSoup(res.text)\n",
    "                title_list = soup.select('#contentarea_left > div.mainNewsList > ul > li > dl')\n",
    "                for title in title_list:\n",
    "                    try:\n",
    "                        news_title = title.select_one('dd.articleSubject').text.strip()\n",
    "\n",
    "                        find_1=news_title.find(\"[\")\n",
    "                        find_2=news_title.find(\"]\")\n",
    "\n",
    "                        if find_1 != -1:\n",
    "                            slice_news_title=news_title[find_1:find_2+1]\n",
    "                        \n",
    "                            news_title.strip(slice_news_title)\n",
    "                            result_list.append([news_title.strip(slice_news_title).strip()])\n",
    "\n",
    "                        elif find_1 == -1:\n",
    "                            result_list.append([news_title])\n",
    "\n",
    "                    except AttributeError:\n",
    "                        news_title = title.select_one('dt.articleSubject').text.strip()\n",
    "                        find_1=news_title.find(\"[\")\n",
    "                        find_2=news_title.find(\"]\")\n",
    "                        if find_1 != -1:\n",
    "                            slice_news_title=news_title[find_1:find_2+1]\n",
    "                        \n",
    "                            news_title.strip(slice_news_title)\n",
    "                            result_list.append([news_title.strip(slice_news_title).strip()])\n",
    "                        elif find_1 == -1:\n",
    "                            result_list.append([news_title])\n",
    "                        \n",
    "                    except:\n",
    "                        error_cnt += 1\n",
    "\n",
    "naver_news_title(date_0)\n",
    "title_df_2 = pd.DataFrame(result_list, columns=['뉴스제목'])\n",
    "title_df_2['주가변동'] = 0\n",
    "result_list = []\n",
    "\n",
    "naver_news_title(date_1)\n",
    "title_df_3 = pd.DataFrame(result_list, columns=['뉴스제목'])\n",
    "title_df_3['주가변동'] = 1\n",
    "result_list = []\n",
    "\n",
    "#네이버 크롤링 데이터 합쳐 csv파일로 만들기\n",
    "title_df2 = pd.concat([title_df_2, title_df_3])\n",
    "#중복 데이터 삭제 \n",
    "del_title_df2 = title_df2.drop_duplicates(['뉴스제목'])\n",
    "title_df2.to_csv('네이버_뉴스타이틀.csv', index=False, encoding='utf-8-sig')\n",
    "title_df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#뉴스데이터 하나로 합치기 \n",
    "naver_data = pd.read_csv('네이버_뉴스타이틀.csv')\n",
    "paxnet_data = pd.read_csv('팍스넷_뉴스타이틀.csv')\n",
    "all_title = pd.concat([naver_data, paxnet_data])\n",
    "all_title.to_csv('paxnet_naver.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22aa38b399a3bdc1f40ec30887608ccf7f07b3d34e65277b4816838b60dccfc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
