{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-1. 형태소 분석\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from konlpy.tag import Kkma        ; kkma = Kkma()\n",
    "from konlpy.tag import Hannanum    ; hannanum = Hannanum()\n",
    "from konlpy.tag import Okt         ; t = Okt()     # 구 트위터\n",
    "from konlpy.tag import *\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# %matplotlib inline\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "plt.rc('font', family='NanumGothic')\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석을 수행할 수 있도록 먼저 데이터를 불러온 후 뉴스제목들을 '\\n' 으로 구분하여 한 문자열에 담아줍니다.\n",
    "\n",
    "news_df = pd.read_csv('paxnet_naver.csv')\n",
    "title_list = news_df.뉴스제목.values.tolist()\n",
    "title_text =  '\\n'.join(title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석을 수행할 수 있도록 먼저 데이터를 불러온 후 뉴스제목들을 '\\n' 으로 구분하여 한 문자열에 담아줍니다.\n",
    "news_df = pd.read_csv('paxnet_naver.csv')\n",
    "title_list = news_df.뉴스제목.values.tolist()\n",
    "title_text =  '\\n'.join(title_list)\n",
    "\n",
    "# 전체 문자열을 형태소 단위로 쪼개어 토큰의 개수를 세어보면 토큰 전체 개수는 373006개, 토큰 unique 개수는 14298개가 출력\n",
    "tokens_ko = t.morphs(title_text)\n",
    "ko = nltk.Text(tokens_ko)\n",
    "# print(len(ko.tokens))          # 토큰 전체 개수\n",
    "# print(len(set(ko.tokens)))\n",
    "\n",
    "stop_words = ['\\n',\"'\",'…',',','[',']','(',')','\"','주','에','코스닥','특징','종목','·','장','코스피','증시','-','적',\\\n",
    "              '도','기술','분석','마감','‘','`','요약','가','’','의','이','오전','★','은','“','대','”','한','B','로',\\\n",
    "              '?','3','선','A','오후','는','5','!','\"…','상','들','1','만에','제','2','…\"','20','일','서','명',\"'…\",'기',\\\n",
    "              '···','10','소','등','으로','자','전','률','미','...','50','세','시','안','폭',\"…'\",'만','9','VI','까지',\\\n",
    "              '눈','더','e','량','고','인','52','성','띄네','1%','부터','다','감','을','지','4','에도','수','7','것','째',\\\n",
    "             '체크','기','···','중','계','관련','왜','1억원','총','내','과','젠','또','연','엔','차','굿모닝','할','8','.',\\\n",
    "             '보다','새','주간','전망','추천','이슈','플러스','사','개월','때','..','임','속','’…','G','나','개','원','에서',\\\n",
    "             '하는','이유','달','→','권','?…','단독','간','배','30','K','저','와','하','/','1조','6','두','해야','분','형',\\\n",
    "             '황','공','&','앞두고','보','문','이번','익','X','1억',']\"','치','산','를','오','해','S','우리','그','된','준','▶',\\\n",
    "             '건','재','반','라','10년','초','3분','월','신','p','급','조','줄','경','했다','구','진','이어','올','발','vs','강',\\\n",
    "             '국','9억','1년','난','판','면','\"(','`…','살','아','인데','번','텍','팜','8월','Q','메','2년','점','하고','10월',\\\n",
    "             'D','비','됐다','채',\"]'\",'보니','손','확','종','동','팔','40','타','~','9월','2100','30%','땐','말','한다','요',\\\n",
    "             \"',\",'스','…`','단','16','길','12','3억','회','될까','호','용','2조','번째','일까','듯','최',\"↑\",\"↓\"]\n",
    "tokens_ko = [each_word for each_word in tokens_ko\n",
    "           if each_word not in stop_words]\n",
    "\n",
    "ko = nltk.Text(tokens_ko)\n",
    "\n",
    "# 그래프에서 한글 폰트가 깨질 경우 실행\n",
    "from matplotlib import font_manager, rc\n",
    "font_name = font_manager.FontProperties(fname='c:/Windows/Fonts/malgun.ttf').get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "ko.plot(50)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ko.vocab().most_common(300)\n",
    "wordcloud = WordCloud(font_path='c:/Windows/Fonts/malgun.ttf',\n",
    "                     relative_scaling=0.2,\n",
    "                     background_color='white').generate_from_frequencies(dict(data))\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석을 위한 함수\n",
    "def tokenizer(text):\n",
    "    okt = Okt()\n",
    "    return okt.morphs(text)\n",
    "\n",
    "\n",
    "def data_preprocessing():\n",
    "    # 수집한 데이터 읽어오기\n",
    "    # news_df = pd.read_excel()\n",
    "    # 학습셋, 테스트셋 분리\n",
    "    title_list = news_df['뉴스제목'].tolist()\n",
    "    price_list = news_df['주가변동'].tolist()\n",
    "    # 데이터의 80%는 학습셋, 20%는 테스트셋\n",
    "    title_train, title_test, price_train, price_test = train_test_split(title_list, price_list, test_size=0.2, random_state=0)\n",
    "    return title_train, title_test, price_train, price_test\n",
    "\n",
    "def learning(x_train, y_train, x_test, y_test):\n",
    "    # 전처리가 끝난 데이터를 단어 사전으로 만들고\n",
    "    # 리뷰별로 나오는 단어를 파악해서 수치화 (벡터화)해서 학습\n",
    "    # tf-idf, 로지스틱 회귀 이용\n",
    "    tfidf = TfidfVectorizer(lowercase=False, tokenizer=tokenizer)\n",
    "    # 로지스틱\n",
    "    logistic = LogisticRegression(C=2, penalty='l2', random_state=0)     # C의 숫자가 너무 크면 과적합 (기본 1), penalty로 과적합 방지\n",
    "    pipe = Pipeline([('vect',tfidf),('clf',logistic)])\n",
    "    # 학습\n",
    "    pipe.fit(x_train, y_train)\n",
    "    # 학습 정확도 측정\n",
    "    y_pred = pipe.predict(x_test)\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    # 학습한 모델을 저장\n",
    "    with open('pipe.dat', 'wb') as fp:     # 쓰기, 바탕화면에 저장됨\n",
    "        pickle.dump(pipe, fp)\n",
    "    print('저장완료')     # 학습된 모델 저장 완료\n",
    "\n",
    "# 학습 함수\n",
    "def model_learning():   # 감성분석 모델 생성\n",
    "    title_train, title_test, price_train, price_test = data_preprocessing()\n",
    "    learning(title_train, price_train, title_test, price_test)\n",
    "\n",
    "model_learning()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def using():\n",
    "    # 객체를 복원, 저장된 모델 불러오기\n",
    "    with open('pipe.dat','rb') as fp:     # 읽기\n",
    "        pipe = pickle.load(fp)\n",
    "    while True :\n",
    "        text = input('뉴스 타이틀을 입력해주세요 : ')\n",
    "        str = [text]\n",
    "        # 예측 정확도\n",
    "        r1 = np.max(pipe.predict_proba(str)*100)     # 확률값을 구해서 *100\n",
    "        # 예측 결과\n",
    "        r2 = pipe.predict(str)[0]     # 긍정('1'), 부정('0')\n",
    "        if r2 == 1:\n",
    "            print('코스피지수는 상승할 것으로 예상됩니다.')\n",
    "        else: \n",
    "            print('코스피지수는 하락할 것으로 예상됩니다.')\n",
    "        print('정확도 : %.3f' % r1)\n",
    "        print('------------------------------------------------')\n",
    "\n",
    "\n",
    "def model_using():   # 감성분석 모델 사용\n",
    "    using()\n",
    "\n",
    "model_using()\n",
    "\n",
    "# 모델 확인에 사용한 뉴스 타이틀\n",
    "# 코스피, 2년 1개월 만에 최저…물가와 수급에 '털썩' #10-13일 뉴스 타이틀 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22aa38b399a3bdc1f40ec30887608ccf7f07b3d34e65277b4816838b60dccfc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
